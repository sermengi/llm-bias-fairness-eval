{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"satoshidg/GSM-MC-Stage\",\n",
    "    data_files=\"test.csv\",\n",
    "    split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(sample):\n",
    "    question = sample[\"Question\"]\n",
    "    choices = {choice: sample[choice] for choice in [\"A\", \"B\", \"C\", \"D\"]}\n",
    "    answer = sample[\"Answer\"]\n",
    "    return question, choices, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question, choices, answer = get_question(dataset[20])\n",
    "print(\"Question:\", question)\n",
    "print(\"Choices:\", choices)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    def __init__(self, dataset_name, data_files=None, split=\"train\", max_samples=None):\n",
    "        self.dataset = load_dataset(dataset_name, data_files=data_files, split=split)\n",
    "        if max_samples:\n",
    "            self.dataset = self.dataset.select(range(max_samples))\n",
    "\n",
    "    def format_sample(self, sample, answer=None):\n",
    "        context = sample.get(\"context\", \"\").strip()\n",
    "        question = sample[\"Question\"]\n",
    "        choices = {k: str(v) for k, v in sample.items() if k in [\"A\", \"B\", \"C\", \"D\"]}\n",
    "        choice_list = \"\\n\".join(\n",
    "            [f\"{option}. {choice}\" for option, choice in choices.items()]\n",
    "        )\n",
    "\n",
    "        prompt = f\"{context}\\n\\nQuestion: {question}\\n\\nChoices:\\n{choice_list}\"\n",
    "\n",
    "        if answer is not None:\n",
    "            prompt += f\"\\n\\nAnswer: {answer}\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def get_sample_prompt(self, index):\n",
    "        sample = self.dataset[index]\n",
    "        prompt = self.format_sample(sample=sample, answer=sample[\"Answer\"])\n",
    "        return prompt\n",
    "\n",
    "    def get_prompts(self):\n",
    "        return [self.format_sample(sample) for sample in self.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_builder = PromptBuilder(\n",
    "    \"satoshidg/GSM-MC-Stage\",\n",
    "    split=\"train\",\n",
    "    data_files=\"test.csv\",\n",
    "    max_samples=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Context Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import ConfigurationManager\n",
    "from src.data_loader import GSM_MC_PromptBuilder\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models import MultipleChoiceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_manager = ConfigurationManager(\n",
    "    config_file_path=\"config.yaml\",\n",
    "    context_config_file_path=\"configs/context_templates.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = config_manager.get_dataset_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_contexts = config_manager.get_contexts_configuration()\n",
    "full_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GSM_MC_PromptBuilder(\n",
    "    dataset_config.dataset_name,\n",
    "    contexts=full_contexts,\n",
    "    split=dataset_config.split,\n",
    "    max_samples=dataset_config.max_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for batch in data_loader:\n",
    "    # `batch` is now a list of dictionaries, ready for processing.\n",
    "    # If your batch_size is 8, batch['prompt'] will be a list of 8 prompts.\n",
    "    prompts_to_send = batch['prompt']\n",
    "    \n",
    "    # Send prompts to your LLM for inference\n",
    "    # llm_responses = your_llm_function(prompts_to_send)\n",
    "    \n",
    "    # For demonstration, let's print the metadata\n",
    "    for i in range(len(prompts_to_send)):\n",
    "        result = {\n",
    "            \"prompt_id\": batch['prompt_id'][i].item(),\n",
    "            \"sample_id\": batch['sample_id'][i].item(),\n",
    "            \"context_category\": batch['context_info']['category'][i],\n",
    "            \"context_name\": batch['context_info']['identity'][i],\n",
    "            \"prompt\": batch['prompt'][i],\n",
    "            \"ground_truth_answer\": batch['answer'][i],\n",
    "            # \"llm_response\": llm_responses[i] \n",
    "        }\n",
    "        all_results.append(result)\n",
    "        print(f\"ID: {result['prompt_id']}, Context: {result['context_name']}, Prompt: {batch['prompt'][i][:30]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultipleChoiceLLM(\n",
    "    model_name=\"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\",\n",
    "    allowed_choices=[],\n",
    "    tokenizer_padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(batch[\"prompt\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/sermengi/llm-bias-fairness-eval/artifacts/predictions.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.context_identity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(prompt) for prompt in df[df.context_identity == \"Asian\"].prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sample_id == 0][[\"context_identity\", \"answer\", \"prediction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
