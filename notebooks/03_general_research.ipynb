{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d01c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(xm.xla_device())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493de80d",
   "metadata": {},
   "source": [
    "## Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of AI is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(xm.xla_device())\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "generated_ids = input_ids\n",
    "\n",
    "for step in range(num_steps):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(generated_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc991189",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "print(\"Generated text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50fe18",
   "metadata": {},
   "source": [
    "## Multiple-Choice Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b09d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
    "\n",
    "Choices:\n",
    "A. 22.0\n",
    "B. 64.0\n",
    "C. 18.0\n",
    "D. 12.0\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(xm.xla_device())\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48504e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get logits for the next token (after the prompt)\n",
    "next_token_logits = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d222ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define allowed answer tokens (A, B, C, D)\n",
    "valid_choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "valid_token_ids = [\n",
    "    tokenizer.encode(choice, add_special_tokens=False)[0] for choice in valid_choices\n",
    "]\n",
    "valid_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.full_like(next_token_logits, float(\"-inf\"))\n",
    "mask[:, valid_token_ids] = next_token_logits[:, valid_token_ids]\n",
    "next_token_id = torch.argmax(mask, dim=-1).unsqueeze(-1)\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode predicted choice\n",
    "predicted_choice = tokenizer.decode(next_token_id.squeeze(), skip_special_tokens=True)\n",
    "print(\"Predicted Answer:\", predicted_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d3b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
