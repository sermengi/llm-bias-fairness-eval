{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(xm.xla_device())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of AI is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(xm.xla_device())\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "generated_ids = input_ids\n",
    "\n",
    "for step in range(num_steps):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(generated_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "print(\"Generated text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Multiple-Choice Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
    "\n",
    "Choices:\n",
    "A. 22.0\n",
    "B. 64.0\n",
    "C. 18.0\n",
    "D. 12.0\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(xm.xla_device())\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get logits for the next token (after the prompt)\n",
    "next_token_logits = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define allowed answer tokens (A, B, C, D)\n",
    "valid_choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "valid_token_ids = [\n",
    "    tokenizer.encode(choice, add_special_tokens=False)[0] for choice in valid_choices\n",
    "]\n",
    "valid_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.full_like(next_token_logits, float(\"-inf\"))\n",
    "mask[:, valid_token_ids] = next_token_logits[:, valid_token_ids]\n",
    "next_token_id = torch.argmax(mask, dim=-1).unsqueeze(-1)\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode predicted choice\n",
    "predicted_choice = tokenizer.decode(next_token_id.squeeze(), skip_special_tokens=True)\n",
    "print(\"Predicted Answer:\", predicted_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Making Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import GSM_MC_PromptBuilder\n",
    "from src.models import MultipleChoiceLLM\n",
    "from src.config import ConfigurationManager\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"config.yaml\"\n",
    "config = ConfigurationManager(config_file_path=config_file_path)\n",
    "dataset_config = config.get_dataset_configuration()\n",
    "model_config = config.get_model_configuration()\n",
    "\n",
    "prompt_builder = GSM_MC_PromptBuilder(\n",
    "    dataset_config.dataset_name,\n",
    "    data_files=dataset_config.data_files,\n",
    "    split=dataset_config.split,\n",
    "    max_samples=dataset_config.max_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_config.model_name\n",
    "allowed_choices = model_config.allowed_choices\n",
    "model = MultipleChoiceLLM(model_name=model_name, allowed_choices=allowed_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = prompt_builder.generate_prompts_and_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sample in outputs:\n",
    "    prompt = sample[\"prompt\"]\n",
    "    prediction = model.predict(prompt)\n",
    "\n",
    "    results.append({\n",
    "    \"sample_id\": sample[\"sample_id\"],\n",
    "    \"question\": sample[\"question\"],\n",
    "    \"choice_A\": sample[\"choices\"].get(\"A\", \"\"),\n",
    "    \"choice_B\": sample[\"choices\"].get(\"B\", \"\"),\n",
    "    \"choice_C\": sample[\"choices\"].get(\"C\", \"\"),\n",
    "    \"choice_D\": sample[\"choices\"].get(\"D\", \"\"),\n",
    "    \"prompt\": sample[\"prompt\"],\n",
    "    \"answer\": sample[\"answer\"],\n",
    "    \"prediction\": prediction,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import ModelInferencePipeline\n",
    "from src.config import ConfigurationManager\n",
    "from src.common import create_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ModelInferencePipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline.run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## DataLoader Implementation (Multi-process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "os.chdir(\"/home/sermengi/llm-bias-fairness-eval\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import GSM_MC_PromptBuilder\n",
    "from src.config import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_manager = ConfigurationManager(\"config.yaml\")\n",
    "dataset_config = config_manager.get_dataset_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_dataset = GSM_MC_PromptBuilder(\n",
    "    dataset_config.dataset_name,\n",
    "    data_files=dataset_config.data_files,\n",
    "    split=dataset_config.split,\n",
    "    max_samples=dataset_config.max_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset=gsm_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"\\n--- Batch {i+1} ---\")\n",
    "    print(f\"  Sample IDs: {batch['sample_id']}\")\n",
    "    \n",
    "    if len(batch['prompt']) > 0:\n",
    "            print(\"\\n  First item in batch:\")\n",
    "            print(f\"    Sample ID: {batch['sample_id'][0]}\")\n",
    "            print(f\"    Question: {batch['question'][0][:100]}...\") # Print first 100 chars\n",
    "            print(f\"    Choices: {batch['choices']['A'][0]}, {batch['choices']['B'][0]}, ...\") # Example of accessing choices\n",
    "            print(f\"    Prompt: {batch['prompt'][0][:150]}...\") # Print first 150 chars\n",
    "            print(f\"    Answer: {batch['answer'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(gsm_dataset) > 0 and i == 0 and len(dataloader) == 0 : # Check if dataloader itself is empty even if dataset is not\n",
    "    print(f\"\\nWarning: Dataset has {len(gsm_dataset)} samples, but DataLoader yielded 0 batches.\")\n",
    "    print(\"This might happen if batch_size > number of samples and drop_last=True, or other DataLoader issues.\")\n",
    "elif i < 3 and i < (len(gsm_dataset) // 2) :\n",
    "    print(f\"\\nNote: Printed {i+1} batches. There might be more batches available in the DataLoader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Model Implementation (Multi-process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import MultipleChoiceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\"\n",
    "allowed_choices = []\n",
    "padding_side = \"left\"\n",
    "\n",
    "llm = MultipleChoiceLLM(\n",
    "            model_name=model_name,\n",
    "            allowed_choices=allowed_choices,\n",
    "            tokenizer_padding_side=padding_side\n",
    "        )\n",
    "print(f\"MultipleChoiceLLM initialized. Using device: {llm.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_text = (\n",
    "        \"Question: What is the capital of France?\\n\"\n",
    "        \"Choices:\\nA. London\\nB. Berlin\\nC. Paris\\nD. Madrid\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "prompt2_text = (\n",
    "    \"Question: Which planet is known as the Red Planet?\\n\"\n",
    "    \"Choices:\\nA. Earth\\nB. Mars\\nC. Jupiter\\nD. Venus\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "prompt3_text = ( # A slightly different style\n",
    "    \"Solve for x: 2x + 3 = 7\\n\"\n",
    "    \"Options:\\nA. 1\\nB. 2\\nC. 3\\nD. 4\\n\"\n",
    "    \"The correct option is: \"\n",
    ")\n",
    "prompts_batch = [prompt1_text, prompt2_text, prompt3_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions = llm.predict(prompts_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prompt = prompt1_text\n",
    "single_prediction = llm.predict(single_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Refactoring Context Generation Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import ConfigurationManager\n",
    "from src.context_generator import ContextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_manager = ConfigurationManager(\n",
    "    config_file_path=\"config.yaml\",\n",
    "    context_config_file_path=\"configs/context_templates.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_config = config_manager.get_contexts_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_generator = ContextGenerator(context_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_generator.generate_contexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_generator.save_generated_contexts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import ConfigurationManager\n",
    "from src.evaluation import ModelEvaluator\n",
    "import pandas as pd\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_manager = ConfigurationManager(\n",
    "    config_file_path=\"configs/config.yaml\",\n",
    "    context_config_file_path=\"configs/context_templates.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelEvaluator:\n",
    "#     def __init__(self, config):\n",
    "#         self.config = config\n",
    "#         self.artifacts_root = self.config.artifacts_root\n",
    "#         self.prediction_file_path = self.config.prediction_file_path\n",
    "#         self.mlflow_run_id = self.config.mlflow_run_id\n",
    "#         self.predictions_df = None\n",
    "\n",
    "#         if not self.mlflow_run_id and not self.prediction_file_path:\n",
    "#             print(\"Either 'mlflow_run_id' or 'prediction_file_path' must be provided in the config.\")\n",
    "        \n",
    "#     def _get_predictions_from_mlflow(self):\n",
    "#         if not self.mlflow_run_id:\n",
    "#             print(\"MLflow Run ID not provided. Skipping MLflow artifact fetching.\")\n",
    "#             return None\n",
    "\n",
    "#         try:\n",
    "#             print(f\"Attempting to fetch predictions from MLflow Run ID: {self.mlflow_run_id}\")\n",
    "#             artifact_uri = f\"runs:/{self.mlflow_run_id}/{self.prediction_file_path}\"\n",
    "#             local_artifact_path = mlflow.artifacts.download_artifacts(\n",
    "#                 artifact_uri=artifact_uri,\n",
    "#             )\n",
    "#             print(f\"Loading predictions from MLflow artifact path: {local_artifact_path}\")\n",
    "#             df = pd.read_csv(local_artifact_path).sort_values(by=\"prompt_id\").set_index(\"prompt_id\")\n",
    "#             return df\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not fetch predictions from MLflow: {e}\")\n",
    "#             return None\n",
    "    \n",
    "#     def _get_predictions_from_local(self):\n",
    "#         if not self.prediction_file_path:\n",
    "#             print(\"Local predictions folder path not provided. Skipping local file loading.\")\n",
    "#             return None\n",
    "        \n",
    "#         if os.path.exists(self.prediction_file_path):\n",
    "#             print(f\"Loading predictions from local path: {self.prediction_file_path}\")\n",
    "#             try:\n",
    "#                 df = pd.read_csv(self.prediction_file_path)\n",
    "#                 return df\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error loading local predictions file '{self.prediction_file_path}': {e}\")\n",
    "#                 return None\n",
    "#         else:\n",
    "#             print(f\"Local predictions file/folder not found at: {self.prediction_file_path}\")\n",
    "#             return None\n",
    "\n",
    "#     def load_predictions(self):\n",
    "#         if self.mlflow_run_id:\n",
    "#             self.predictions_df = self._get_predictions_from_mlflow()\n",
    "        \n",
    "#         if self.predictions_df is None:\n",
    "#             self.predictions_df = self._get_predictions_from_local()\n",
    "        \n",
    "#         if self.predictions_df is None:\n",
    "#             raise RuntimeError(\"Failed to load predictions from both MLflow and local paths.\")\n",
    "\n",
    "#         required_columns = [\"context_category\", \"context_identity\", \"answer\", \"prediction\"]\n",
    "#         if not all(col in self.predictions_df.columns for col in required_columns):\n",
    "#             raise ValueError(f\"Missing one or more required columns in predictions file. \"\n",
    "#                              f\"Expected: {required_columns}, Found: {self.predictions_df.columns.tolist()}\")\n",
    "\n",
    "#         return self.predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = ModelEvaluator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.load_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
