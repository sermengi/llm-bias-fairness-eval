{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d01c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(xm.xla_device())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493de80d",
   "metadata": {},
   "source": [
    "## Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of AI is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(xm.xla_device())\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "generated_ids = input_ids\n",
    "\n",
    "for step in range(num_steps):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(generated_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc991189",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "print(\"Generated text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50fe18",
   "metadata": {},
   "source": [
    "## Multiple-Choice Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b09d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
    "\n",
    "Choices:\n",
    "A. 22.0\n",
    "B. 64.0\n",
    "C. 18.0\n",
    "D. 12.0\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(xm.xla_device())\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48504e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get logits for the next token (after the prompt)\n",
    "next_token_logits = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d222ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define allowed answer tokens (A, B, C, D)\n",
    "valid_choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "valid_token_ids = [\n",
    "    tokenizer.encode(choice, add_special_tokens=False)[0] for choice in valid_choices\n",
    "]\n",
    "valid_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.full_like(next_token_logits, float(\"-inf\"))\n",
    "mask[:, valid_token_ids] = next_token_logits[:, valid_token_ids]\n",
    "next_token_id = torch.argmax(mask, dim=-1).unsqueeze(-1)\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode predicted choice\n",
    "predicted_choice = tokenizer.decode(next_token_id.squeeze(), skip_special_tokens=True)\n",
    "print(\"Predicted Answer:\", predicted_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e4190",
   "metadata": {},
   "source": [
    "## Making Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import GSM_MC_PromptBuilder\n",
    "from src.models import MultipleChoiceLLM\n",
    "from src.config import ConfigurationManager\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"config.yaml\"\n",
    "config = ConfigurationManager(config_file_path=config_file_path)\n",
    "dataset_config = config.get_dataset_configuration()\n",
    "model_config = config.get_model_configuration()\n",
    "\n",
    "prompt_builder = GSM_MC_PromptBuilder(\n",
    "    dataset_config.dataset_name,\n",
    "    data_files=dataset_config.data_files,\n",
    "    split=dataset_config.split,\n",
    "    max_samples=dataset_config.max_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44566cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_config.model_name\n",
    "allowed_choices = model_config.allowed_choices\n",
    "model = MultipleChoiceLLM(model_name=model_name, allowed_choices=allowed_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413839a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = prompt_builder.generate_prompts_and_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sample in outputs:\n",
    "    prompt = sample[\"prompt\"]\n",
    "    prediction = model.predict(prompt)\n",
    "\n",
    "    results.append({\n",
    "    \"sample_id\": sample[\"sample_id\"],\n",
    "    \"question\": sample[\"question\"],\n",
    "    \"choice_A\": sample[\"choices\"].get(\"A\", \"\"),\n",
    "    \"choice_B\": sample[\"choices\"].get(\"B\", \"\"),\n",
    "    \"choice_C\": sample[\"choices\"].get(\"C\", \"\"),\n",
    "    \"choice_D\": sample[\"choices\"].get(\"D\", \"\"),\n",
    "    \"prompt\": sample[\"prompt\"],\n",
    "    \"answer\": sample[\"answer\"],\n",
    "    \"prediction\": prediction,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2488d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import ModelInferencePipeline\n",
    "from src.config import ConfigurationManager\n",
    "from src.common import create_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ModelInferencePipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline.run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d1293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import GSM_MC_PromptBuilder\n",
    "from src.config import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b813a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_manager = ConfigurationManager(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_manager.get_dataset_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_builder = GSM_MC_PromptBuilder(config.dataset_name, config.data_files, config.split, config.max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd1379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
